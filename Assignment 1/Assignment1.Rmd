---
title: "Assignment 1"
subtitle: "Dataset Selection and Initial Processing"
author: "Tony Xie"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: true
    toc_depth: 3
    fig_caption: yes
bibliography: assignment1.bib
csl: biomed-central.csl
---

# Selecting the Dataset

### My area of interest (for this project)
Alzheimer's disease is neurodegenerative disorder that affects millions worldwide. There is currently no cure, and few existing treatments slow the progression of the disease [@scheltens_2021]. The heritability of Alzheimer's disease has been estimated to be between 60-80%, with over 40 Alzhiemer's disease-associated genetic risk loci[@scheltens_2021]. 

For this course, I wanted to analyze an transcriptomic dataset relevant to the disease, and decided to search the GEO database using the keyword "Alzheimer's" for a dataset investigating the effect of particular Alzheiemer's-associated mutations. To only observe bulk RNA-seq human datasets, I used the filters "Expression profiling by high throughput sequencing" and "Homo sapiens" for study type and organism, respectively, as shown in class.

I eventually came across a dataset (GSE266358) from a paper investigating the effects of a specific PSEN1 mutation on gene expression in brain organoids (among other conditions)[@majernikova_2024]. PSEN1 is a part of gamma secretase, a transmembrane protein complex which carries out proteolytic cleavage, and mutations in PSEN1 have been heavily implicated in various cases of early onset Alzheimer's disease. [@bagaria_2022] Thus, the dataset interested me greatly. 

### The control and test conditions of the dataset
The dataset contains samples from many different conditions ((BACE-1 inhibition, ferrostatin, co-culture with congenic microglia, iPSC vs brain organoid); in total, there are 32 samples, with 4 replicates per condition. For this particular report, I wanted to narrow my focus on untreated brain organoids, the test condition being the presence of the PSEN1 Delta-E9 mutation (with 4 replicates) and the control condition being isogenic controls generated by CRISPR-Cas9 (also with 4 replicates). Thus, I will be working with 8 total samples.

# Initial Processing

First, I added the necessary packages to the workspace that are needed for the report:
```{r, message=FALSE}
library("GEOquery")
# for querying and downloading from the GEO database

library("edgeR")
# for filtering and normalization

library("biomaRt")
# for mapping ensembl accessions to HUGO symbols
```

GEOQuery [@davis_2007], EdgeR [@robinson_2010] and biomaRt [@durinck_2009] are cited in the reference list. 

Acquiring general information about the GEO dataset:
```{r message=FALSE}

# GEO series ID for the dataset of interest
alzheimers_geoid <- "GSE266358"

gse <- getGEO(alzheimers_geoid, GSEMatrix=FALSE)
```

Here's the summary of the file:
```{r message = FALSE}
gse@header$summary
```
As expected, the summary indicates a gene expression dataset for brain organoids harboring the PSEN1 Delta-E9 mutation and their isogenic controls, among a myriad of other conditions. 

### Collecting Sample Annotation Data

Now, we can acquire data for each GEO sample in the dataset and filter specifically for the test condition of untreated brain organoids with the PSEN1 Delta-E9 mutation and the control condition of their healthy, isogenic controls.

First:
```{r}
# Get a list of the sample annotation data 
list_of_samples <- gse@gsms

length(list_of_samples)
```

There are 32 of them, matching the description of the dataset on the GEO site. Formatting the data into a data frame:

```{r}

# formatting the relevant info into a data frame, one row per sample:
sample_info <- do.call(rbind, lapply(list_of_samples,function(x){c(x@header$title, x@header$characteristics_ch1)}))
sample_info <- as.data.frame(sample_info)

# naming the columns (note that GSM identifiers are the row names)
colnames(sample_info) <- c("header", "sex", "cell_type", "genotype", "treatment")
```

And printing it out as a table:
```{r, echo=FALSE}

knitr::kable(sample_info, type = "html", caption = "Table 1: All Sample Annotations for GEO Dataset GSE266358")
```

We do not need all columns (only cell type, genotype and treatment are relevant 
for us).
```{r}
# selecting the columns containing cell type, genotype and treatment information
sample_info <- sample_info[,c(3, 4, 5)]
```

There are many samples. We want to focus only on untreated brain organoids:
```{r}

# removing the unncessary parts of descriptions for easier searching
sample_info[,"cell_type"] <- gsub("cell type: ", "", sample_info[,"cell_type"])
sample_info[,"genotype"] <- gsub("genotype: ", "", sample_info[,"genotype"])
sample_info[,"treatment"] <- gsub("treatment: ", "", sample_info[,"treatment"])

# filtering for untreated brain organoid samples:
sample_info <- sample_info[sample_info$cell_type == "Brain Organoid" & 
                           sample_info$treatment == "Untreated",]
```

Here's the summary of the final samples to keep:
```{r echo=FALSE}
knitr::kable(sample_info, type="html", caption="Table 2: Annotations for samples of interest, notably untreated brain organoids with the PSEN1 Delta-E9 genotype and its isogenic control)")
```

### Downloading the data

I got the name of the supplementary file here:
```{r message = FALSE}

sfilenames = getGEOSuppFiles(alzheimers_geoid, 
                             fetch_files = FALSE)
sfilenames$fname
```

It's a .tar file, meaning that we'll have to extract it after downloading it. Note that the extracted data will contain 32 text files, each corresponding to a single sample. Based on the original paper, it appears that the data they have provided are **raw counts** [@majernikova_2024]. We need to select the files containing data for the controls/test conditions we are interested in, then concatenate the data in the files into a single data frame for downstream processing. Here's the entire process I took to achieve this:
```{r message = FALSE}

# set the directory for download
setwd("/home/rstudio/projects")

# we are interested in the first (and only) supplemental file
data_filename <- sfilenames$fname[1]

# if we have not already undergone the process of downloading and extracting the 
# data (and have not saved the data in an .RData file titled "raw_counts")
if (! file.exists("raw_counts.RData")) {
  
  # retrieve the .tar file
  sfiles = getGEOSuppFiles(alzheimers_geoid, 
                           filter_regex = data_filename,
                           fetch_files = TRUE)
  
  # get the names of the files that would be in the extracted from the tar file
  all_files <- untar(file.path(alzheimers_geoid, data_filename), list=TRUE)
  
  # Note that there are 32 possible files that would be extracted from the tar 
  # file, each file corresponding to a single sample. We'll need to select only
  # the files corresponding to samples we are interested in, then concatenate 
  # the data from all of these samples together into one data frame
  
  # ids for samples of interest (retrieved from sample annotations in an above
  # code block -- note that we are only interested in untreated brain 
  # organoids under various conditions) 
  gsms_of_interest <- rownames(sample_info)
  
  # get the names of the files of interest to retrieve (corresponding to 
  # samples of interest)
  files_of_interest <- all_files[grepl(pattern=paste(gsms_of_interest, 
                                                     collapse="|"),
                                       all_files)]
  
  # extract only these files from the .tar file, and place them in a new
  # directory titled "GSE266358_extracted" (named by the GEO series id of the 
  # dataset)
  untar(file.path(alzheimers_geoid, sfilenames$fname), files = files_of_interest,
  exdir = "GSE266358_extracted")
  
  # read the first file of interest
  raw_count_data <- read.delim(sep="\t",
      file.path("GSE266358_extracted", files_of_interest[1]),
      header=FALSE,
      check.names=TRUE)
  
  # keep only the first and third columns, which contain the ensembl identifier 
  # and a single column of expression data for the sample (the second column 
  # seems to contain 
  # gene genes, which we will map ourselves from the given ensembl ids)
  raw_count_data <- raw_count_data[,c(1,3)]
  
  # label the columns appropriately (the expression data by the sample 
  # identifer)
  colnames(raw_count_data) <- c("ensembl_gene_id", 
                                gsub(pattern="_.*", replacement="",
                                     files_of_interest[1]))
  
  # read the other files of interest (we know there are eight samples of 
  # interest, so "length(files_of_interest)" will never be 1)
  for (file in files_of_interest[2:length(files_of_interest)]){
    
    more_data <- read.delim(sep="\t",
      file.path("GSE266358_extracted", file),
      header=FALSE,
      check.names=TRUE)[,c(1,3)]
      
    colnames(more_data) <- c("ensembl_gene_id", 
                              gsub(pattern="_.*", replacement="", file))
    
    # merge the data together by ensembl identifier
    raw_count_data <- merge(raw_count_data, more_data, 
                            by.x = 1, by.y = 1, 
                            all.x = TRUE, all.y = TRUE)
      
  }
  
  # set the row names to the ensembl identifiers
  rownames(raw_count_data) <- raw_count_data[,1]
  # remove the ensembl identifer column (it's still present in the row names)
  raw_count_data <- raw_count_data[, 2:ncol(raw_count_data)]
  
  # save the raw count data in an .RData file (so we don't have to redownload 
  # the data every time)
  save(raw_count_data, file = "raw_counts.RData")
  
} else {
  
  # if we've gone through this process before, simply load the raw counts data
  load("raw_counts.RData")
}

```

### Generating Initial Overview Statistics

Prior to any filtering, our data has the following numbers of rows and columns:
```{r}
dim(raw_count_data)
```
Suggesting an initial coverage of over 60,000 genes across the 8 samples. This is very large... 

We define a function for plotting densities by sample, which we'll call on a couple times in this report. The code here is heavily based on the code shown in class (Lecture 4).
```{r}

plot_densities_by_sample <- function(dat, xlab, ylab, main){
  
  # first, apply log to each column (each representing a sample), and generate
  # density statistics for each
  densities_by_sample <- apply(log2(dat), 2, density)
  
  # set optimal minimum/maximum window limits
  x_lim <- 0
  y_lim <- 0
  for (i in seq_along(densities_by_sample)){
    # iteratively checks the min/max and updates if new ones are found
    x_lim <- range(c(x_lim, densities_by_sample[[i]]$x))
    y_lim <- range(c(y_lim, densities_by_sample[[i]]$y))
  }
  
  # set the colours to the colours of the rainbow
  cols <- rainbow(length(densities_by_sample))
  
  # set the lines
  ltys <- rep(1, length(densities_by_sample))
  
  # plot the density distribution for the first sample
  plot(densities_by_sample[[1]], xlim=x_lim, ylim=y_lim, 
       type="n", ylab=ylab, xlab=xlab, main=main)
  
  # plot the rest of the density distributions
  for (i in seq_along(densities_by_sample)){
    lines(densities_by_sample[[i]], col=cols[i])
  }
  
  # generate a legend in the top right
  legend("topright", colnames(dat), col=cols, lty=ltys, cex = 0.75)
}

```

Before further processing, we'll format the raw count data into a count matrix: 
```{r}
# matrix conversion
raw_count_matrix <- as.matrix(raw_count_data)
```

And also create a new matrix with raw counts converted to counts-per-million to account for any library size differences.
```{r}
# cpm conversion using edgeR
initial_cpm <- cpm(raw_count_matrix)
```

Generating a density plot for the eight samples of interest, we see:
```{r fig.cap="*Figure 1: Smoothing density plot of initial counts, converted to log-2 counts per million, prior to filtering or normalization. Eight samples are present, four for untreated brain organoids with the PSEN1 Delta-E9 mutation, and four for their isogenic controls.*"}

plot_densities_by_sample(initial_cpm, 
                         xlab="log2-CPM", 
                         ylab="Smoothing Density", 
                         main="Density of Initial Counts (in CPM) Prior to Filtering and Normalization")
```


For future reference (and also referring to table 2), the GSM ids ending in 0, 1, 2, and 3 refer to untreated brain organoids with the PSEN1 Delta-E9 mutation, whereas the ids ending in 6, 7, 8 and 9 refer to their isogenic controls [@majernikova_2024]. 

Based on the density plot of the initial counts, there appears to be a lot of extremely lowly expressed genes, which could pose a problem for our downstream gene expression analysis if we don't filter them out.

### Filtering out lowly expressed genes (and discussion of outliers)

The initial paper [@majernikova_2024] filtered for genes with at least 0.5 CPM in at least three samples. Aside from that, they do not mention removing any other outliers. Testing out this filter:

```{r}
# checking each row (corresponding to a gene) to see if at least three samples 
# have at least 0.5 cpm  
filtered_counts_initial_paper <- 
  raw_count_matrix[apply(cpm(raw_count_matrix), 
                         1, 
                         function(x){sum(x > 0.5) >= 3}),]
```

Visualizing the filtered counts by the initial paper's metrics:
```{r, fig.cap="*Figure 2: Smoothing density plot of filtered counts based on a minimum of 0.5 CPM for at least three samples, converted to log-2 counts per million, prior to normalization. Eight samples are present, four for untreated brain organoids with the PSEN1 Delta-E9 mutation, and four for their isogenic controls.*"}

plot_densities_by_sample(cpm(filtered_counts_initial_paper), 
                         xlab="log2-CPM", 
                         ylab="Smoothing Density", 
                         main="Count Density After Filtering for 0.5 CPM in >= 3 Samples")
```


The data still does not seem as clean as we might like; the "second bump" at approximately x=0 (CPM around 1) is highly apparent. Checking our gene coverage, we see:
```{r}
dim(filtered_counts_initial_paper)
```
that around 17,000 genes remain after the filter. 

As stated in class, the edgeR protocol [@anders_2013] recommends that we remove genes that are not well-expressed in all samples for any condition. Based on this, I tried a slightly more stringent filter for genes with at least 1 cpm in at least 4 samples:  

```{r, fig.cap="*Figure 3: Smoothing density plot of filtered counts based on a minimum of 1 CPM for at least four samples (inspired by the edgeR protocol), converted to log-2 counts per million, prior to normalization. Eight samples are present, four for untreated brain organoids with the PSEN1 Delta-E9 mutation, and four for their isogenic controls.*"}

# checking each row (corresponding to a gene) to see if at least four samples 
# have at least 1 cpm
filtered_counts_edgeR_protocol <- 
  raw_count_matrix[apply(cpm(raw_count_matrix), 
                         1, 
                         function(x){sum(x > 1) >= 4}),]

plot_densities_by_sample(cpm(filtered_counts_edgeR_protocol), 
                         xlab="log2-CPM", 
                         ylab="Smoothing Density", 
                         main="Count Density After Filtering for 1 CPM in >= 4 Samples")
```

The data definitely appears a little cleaner. In terms of our coverage after using this slightly more stringent filter:
```{r}
dim(filtered_counts_edgeR_protocol)
```
We see that over 15000 genes still remain. For now, let's use this filter.


To calculate the number of genes we have removed in the filtering process, we get:
```{r}
nrow(raw_count_data) - nrow(filtered_counts_edgeR_protocol)
```

This is a pretty enormous number, but as we started off with more 60,000 genes, 
it seems fair.

# Normalization

## Pre Normalization

To observe the density plot of the data prior to normalization (but after filtering), see the above section. Now, we generate a box plot of the filtered data prior to normalization as follows:

```{r warning=FALSE, fig.cap="*Figure 4: Boxplot of filtered counts based on a minimum of 1 CPM for at least four samples (inspired by the edgeR protocol), converted to log-2 counts per million, prior to normalization. Eight samples are present, the first four for untreated brain organoids with the PSEN1 Delta-E9 mutation, and latter four for their isogenic controls.*"}
# Note that cpm values of 0 (and therefore an undefined log value) will by
# default not be plotted

# get the log of the cpm
log_filtered_cpm <- log2(cpm(filtered_counts_edgeR_protocol))

# generate the boxplot
boxplot(log_filtered_cpm, xlab = "", ylab = "log2 CPM", 
        las = 2, cex = 0.5, cex.lab = 1, cex.axis = 0.5, 
        main = "Boxplots of Counts After Filtering But Before Normalization")

# adjust the x axis label
mtext("Samples", side=1, line=4)

```

Suprisingly, the data already appears to be fairly well-distributed in relation to each other, with the median values of each sample already roughly lined up. 

## Normalizing the dataset

Let's apply TMM, one of the specialized normalization approaches for RNA-seq. As covered in class, TMM is sample-based and assumes that most genes are not differentially expressed (and that the data is balanced).

```{r}
# create the DGEList object required for edgeR's TMM normalization
d <- edgeR::DGEList(counts=filtered_counts_edgeR_protocol, group=sample_info$genotype)
# samples are grouped by genotype

# calculate the normalization factors
d <- edgeR::calcNormFactors(d)

# get the normalized counts per million
normalized_counts <- edgeR::cpm(d)
```

## Post-Normalization

After normalization, we can observe the distribution of the data via boxplots here: 

```{r warning=FALSE, fig.cap="*Figure 5: Boxplot of filtered counts based on a minimum of 1 CPM for at least four samples (inspired by the edgeR protocol), converted to log-2 counts per million, after normalization. Eight samples are present, the first four for untreated brain organoids with the PSEN1 Delta-E9 mutation, and latter four for their isogenic controls.*"}

# get the log of the cpm
log_normalized_cpm <- log2(normalized_counts)

# generate the boxplot
boxplot(log_normalized_cpm, xlab = "", ylab = "log2 CPM", 
        las = 2, cex = 0.5, cex.lab = 1, cex.axis = 0.5, 
        main = "Boxplots of Counts After Normalization")

# adjust the x axis label
mtext("Samples", side=1, line=4)

```

As noted in class, TMM normalization often causes the sample medians to become closer together, although this feature is not directly intentional (as would be the case for quantile normalization). We see that this appears to be the case with the above box plot, which is a good sign. The changes caused by normalization isn't drastic (as expected, given how neat the original data was), but it's there.

We can also generate a density plot as follows:

```{r, fig.cap="*Figure 6: Smoothing density plot of filtered counts based on a minimum of 1 CPM for at least four samples (inspired by the edgeR protocol), converted to log-2 counts per million, after normalization. Eight samples are present, four for untreated brain organoids with the PSEN1 Delta-E9 mutation, and four for their isogenic controls.*"}
plot_densities_by_sample(normalized_counts, 
                         xlab="log2-CPM", 
                         ylab="Smoothing Density", 
                         main="Count Density After Normalization")
```

To be completely honest, there doesn't seem to be very much of a change at all, suggesting that original dataset had very low technical variation.

Here, we create an MDS plot to see if the samples from each condition (again, mutants vs isogenic controls) cluster together: 
```{r, fig.cap="*Figure 7: Multi dimensional scaling plot of filtered counts based on a minimum of 1 CPM for at least four samples (inspired by the edgeR protocol) after normalization. Eight samples are present, each represented by a dot. Four untreated brain organoid samples with the PSEN1 Delta-E9 mutation are coloured blue, while their isogenic controls are coloured green.*"}
# we colour the samples green or blue, depending on the genotype (and thus
# condition)
limma::plotMDS(d, labels=NULL, 
               main="MDS Plot of PSEN1 Mutant Samples and their Isogenic Controls",
               pch=1, cex = 1.5, 
               col=c("blue", "darkgreen")[factor(sample_info$genotype)])

# we create a legend for the data
legend("topright", legend=levels(factor(sample_info$genotype)), pch=c(1), col= c("blue","darkgreen"), bty="n", cex=0.75)

```

It appears the samples from each condition do in fact cluster together and are highly distinct from the samples in the other condition. This is a good sign for differential gene expression that we'll conduct down the line. 


Now we determine the dispersion and generate a BCV plot, with groups determined by condition, as follows:
```{r, fig.cap="*Figure 8: BCV plot of normalized counts to visualize the biological coefficient of variation across different mean expressions. Grouping is based on genotype (PSEN1 Delta-E9 mutation, isogenic controls). Tagwise dispersion is represented by black dots, while the common dispersion and trend lines are red and bue, respectively.*"}
# model design should be by genotype (PSEN1 mutation vs isogenic control)
model_design <- model.matrix(~sample_info$genotype)

# estimate the dispersion (given that technical variation should be accounted 
# for, this should only be biological variation)
d <- edgeR::estimateDisp(d, model_design)

# create BCV plot
edgeR::plotBCV(d,
               col.tagwise = "black",
               col.common = "red", 
               main="BCV Plot for Normalized Counts Given Genotype Grouping")
```

As expected, the dispersion decreases as the average number of counts increases.

As the downstream analysis using edgeR will assume that the data follows a negative bionomial distribution, we generate this mean/variance plot to see if our data follows the assumption:
```{r warning=FALSE, fig.cap="*Figure 9: Mean-variance plot of filtered and normalized count data to visualize whether the data follows the negative binomial distribution. Such is an assumption required for the accuracy of downstream analysis using edgeR. Red crosses indicates common dispersion, blue dots indicate tagwise dispersion, grey dots indicate raw data, and the blue line is the negative binomial line.*"}

edgeR::plotMeanVar(d, show.raw.vars = TRUE, 
                   show.tagwise.vars=TRUE, NBline=TRUE, 
                   show.ave.raw.vars = TRUE, 
                   show.binned.common.disp.vars = TRUE,
                   main="Mean-Variance Plot of Normalized Count Data")

```

The data (raw, tagwise, and common) all roughly follow the negative binomial line, suggesting that the assumption is followed. 

# Identifier mapping from Ensembl to Hugo

### Initial Logistics

First, we configure the settings for biomaRt to access the human database related to ensembl: 

```{r}
# set the mart to ensembl
ensembl <- useMart("ensembl")

# use the human dataset
# I know the dataset name from the lectures in class, but it is fairly 
# easy to search them up as well
ensembl_human <- useDataset("hsapiens_gene_ensembl", mart=ensembl)
```

We get all the filters in the ensembl human mart and search for the ones related to ensembl (we want to filter by ensembl gene id):
```{r}
# get all filters in ensembl human
biomart_human_filters <- listFilters(ensembl_human)

# look for filters with "ensembl" in the filter name
ensembl_filters <- biomart_human_filters[grep(pattern="ensembl",
                           biomart_human_filters$name),]
```

To not clutter the report, I have opted not to print out all of the filters with "ensembl" in the filter name. The one relevant to us is titled "ensembl_gene_id". 

Next, we can search for the relevant attributes to the ensembl and hgnc identifiers:
```{r}
ensembl_attributes <- searchAttributes(mart = ensembl_human, "ensembl_gene_id")
hgnc_attributes <- searchAttributes(mart = ensembl_human, "hgnc")
```

Again, they're not printed out for brevity's sake, but the attributes that are relevant for mapping ensembl to hgnc ids are "ensembl_gene_id" and "hgnc_symbol", respectively. 

Now, since we know the relevant filter and attribute names, we can map ensembl identifiers to hgnc identifiers:
```{r}
# get the ensembl identifiers from the rownames column
ids_to_convert <- rownames(normalized_counts)

# call on biomaRt to retrieve the ensembl - hgnc mappings
id_conversion <- getBM(attributes = c("ensembl_gene_id", "hgnc_symbol"),
                       filters = c("ensembl_gene_id"), 
                       values = ids_to_convert, 
                       mart = ensembl_human)
```

And add the new HGNC symbols to our counts data:
```{r}
# merge the ensembl -> hgnc id data frame with the counts data frame via
# ensembl id
normalized_counts_annot <- merge(id_conversion, 
                                 normalized_counts, 
                                 by.x = 1, by.y = 0, all.y = TRUE)
```

### Were there expression values that could not be mapped to current HUGO symbols?

To answer this question, let's first identify the number of rows where the HGNC symbol is "NA": 
```{r}
sum(is.na(normalized_counts_annot$hgnc_symbol))
```

Here are the first 10 ensembl ids where such is the case:
```{r}
normalized_counts_annot[
  is.na(normalized_counts_annot$hgnc_symbol),][1:10,"ensembl_gene_id"]
```
After doing a manual search in the Ensembl web interactive database [@harrison_2024], it appears all of these identifiers are no longer in use. 83 unmappable ensembl ids doesn't seem too bad. The rest of the IDS are up-to-date, which makes sense given that the paper far the dataset was published fairly recently. Let's remove all 83 from our count data:

```{r}
normalized_counts_annot_mapped <-
  normalized_counts_annot[!is.na(normalized_counts_annot$hgnc_symbol),]

```

Aside from ensembl identifiers that do not map to any HGNC symbols, I also noticed quite a few ensembl identifiers that mapped to "blank" ones as follows:

```{r}
sum(normalized_counts_annot_mapped$hgnc_symbol == "")
```

Investigating this further, here are the first 10 ensembl identifiers that mapped to "blank" hgnc identifiers:

```{r}
normalized_counts_annot_mapped[
  normalized_counts_annot_mapped$hgnc_symbol == "",
][1:10, 1]
```

All of these identifiers are still in commission. Let's check their biological function. Retrieving the gene types for each of them:

```{r}
# get all ensembl ids with blank HGNC symbols
ensembl_ids_with_blank_hgncs <- normalized_counts_annot_mapped[
  normalized_counts_annot_mapped$hgnc_symbol == "",
][, 1]

gene_types_of_blank_hgncs <- getBM(attributes = c("ensembl_gene_id",
                                                  "gene_biotype"),
                                   filters = c("ensembl_gene_id"), 
                                   values = ensembl_ids_with_blank_hgncs, 
                                   mart = ensembl_human)
```

And outputting them as a basic table:
```{r}
# acquiring the table and converting it to a data frame for naming
gene_types <- data.frame(table(gene_types_of_blank_hgncs[,2]))

# renaming the columns
colnames(gene_types) <- c("Gene Type", "Frequency")

# outputting the table
knitr::kable(gene_types, type="html", caption="Table 3: Gene Types Corresponding to Ensembl Identifiers with \"Blank\" HGNC Symbols")
```

The vast majority of the "blank HGNC" ensembl identifiers seem to be non-protein coding genes (in particular, long non-coding RNAs and pseudo genes) or ones that have yet to be experimentally determined to be protein coding (note that TEC stands for "to be experimentally confirmed"). For now, we discard them here:

```{r}
normalized_counts_annot_mapped <- 
  normalized_counts_annot_mapped[
    normalized_counts_annot_mapped$hgnc_symbol != "",]
```

After removing all unmapped ensembl identifiers, we have the following coverage:
```{r}
dim(normalized_counts_annot_mapped)
```

### Were there expression values that were not unique to specific genes?

First, let's see the number of unique HUGO identifiers in our coverage:
```{r}
length(unique(normalized_counts_annot_mapped$hgnc_symbol))
```

And now, the total length:
```{r}
length(normalized_counts_annot_mapped$hgnc_symbol)
```

They differ by 1. Let's find the single duplicated HUGO symbol:
```{r}
normalized_counts_annot_mapped$hgnc_symbol[
  duplicated(normalized_counts_annot_mapped$hgnc_symbol) == TRUE]
```

And let's find the rows corresponding to this duplication:
```{r}
knitr::kable(normalized_counts_annot_mapped[
  normalized_counts_annot_mapped$hgnc_symbol == "GUSBP11",
], type="html", caption="Table 4: Rows with the GUSBP11 HUGO symbol among the annotated, normalized count data")
```
Manually searching the ensembl database, the first ensembl id corresponds to a gene page with 289 transcripts, while the second corresponds to a page with only one transcript variant [@harrison_2024]. The genes map to the same part of the human genome, and the second entry seems to just be a splicing variant that has not yet been incorporated in the main page (the transcript ID tied to the page is unique). Thus, we'll solve this by summing up their expression values in one row and removing the other row as follows:

```{r}

# get the indices of the rows of interest
relevant_rows <- which(normalized_counts_annot_mapped$hgnc_symbol == "GUSBP11")

# sum up the expression values (right now, the first two columns are the 
# ensembl ID and HGNC symbols)
for (i in 3:10){
  normalized_counts_annot_mapped[relevant_rows[1],i] <- 
    normalized_counts_annot_mapped[relevant_rows[1],i] + 
    normalized_counts_annot_mapped[relevant_rows[2],i]
}

# remove the duplicate column
normalized_counts_annot_mapped <- 
  normalized_counts_annot_mapped[-relevant_rows[2],]

```

### How did you handle replicates? 

Replicates for each sample condition are not merged (yet). We end off with four columns per condition (one replicate corresponding to each column), with a total of eight columns.

# What is the final coverage of your dataset?

Let's first set the HGNC symbols as our row names and remove the ensembl 
id/HGNC symbol columns to get the desired format:
```{r}
# row renaming
rownames(normalized_counts_annot_mapped) <- 
  normalized_counts_annot_mapped$hgnc_symbol

# removing the first two columns, which correspond to ensembl ids and HGNC 
# symbols
final_processed_count_data <- normalized_counts_annot_mapped[,3:10]
```

And for the coverage, we see:
```{r}
dim(final_processed_count_data)
```
We have roughly 14000 genes remaining for downstream analysis. 
